{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from bindenergy import *\n",
    "#%env TORCH_EXTENSIONS_DIR=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = \"EVQLVESGGGLVQAGDSLRLSCTASG\"\n",
    "frame2 = \"MGWFRQAPGKEREFVASIS\"\n",
    "frame3 = \"TYYADSVKGRFTISRDDARNTVYLQMNSLKPEDTAVYYCNM\"\n",
    "frame4 = \"EYWGQGTQVTVSS\"\n",
    "\n",
    "def has_no_repeat(seq):\n",
    "    for aa in ALPHABET[1:]:\n",
    "        if aa * 3 in seq:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DSMBind Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceModel(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.encoder = SRUpp(\n",
    "                len(ALPHABET),\n",
    "                args.hidden_size // 2,\n",
    "                args.hidden_size // 2,\n",
    "                num_layers=args.depth,\n",
    "                dropout=args.dropout,\n",
    "                bidirectional=True,\n",
    "        )\n",
    "        self.W_o = nn.Sequential(\n",
    "                nn.Linear(args.hidden_size, args.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(args.hidden_size, 1)\n",
    "        )\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, cdr_S, label):\n",
    "        cdr_mask = (cdr_S > 0).float()\n",
    "        cdr_S = F.one_hot(cdr_S, num_classes=len(ALPHABET)).float()\n",
    "        cdr_h, _, _ = self.encoder(\n",
    "                cdr_S.transpose(0, 1),\n",
    "                mask_pad=(~cdr_mask.transpose(0, 1).bool())\n",
    "        )\n",
    "        cdr_h = cdr_h.transpose(0, 1)  # [B, N, H]\n",
    "        cdr_h = cdr_h.mean(dim=1)\n",
    "        logit = self.W_o(cdr_h).squeeze(-1)\n",
    "        loss = self.bce_loss(logit, label)\n",
    "        return loss, torch.sigmoid(logit)\n",
    "    \n",
    "    def predict(self, cdr_S):\n",
    "        cdr_mask = (cdr_S > 0).float()\n",
    "        cdr_S = F.one_hot(cdr_S, num_classes=len(ALPHABET)).float()\n",
    "        cdr_h, _, _ = self.encoder(\n",
    "                cdr_S.transpose(0, 1),\n",
    "                mask_pad=(~cdr_mask.transpose(0, 1).bool())\n",
    "        )\n",
    "        cdr_h = cdr_h.transpose(0, 1)  # [B, N, H]\n",
    "        cdr_h = cdr_h.mean(dim=1)\n",
    "        logit = self.W_o(cdr_h).squeeze(-1)\n",
    "        return torch.sigmoid(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt, model_args = torch.load('ckpts/PDL1.ckpt')\n",
    "model = SequenceModel(model_args).cuda()\n",
    "model.load_state_dict(model_ckpt)\n",
    "model.eval()\n",
    "aaratio = torch.load('ckpts/aaratio.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100000):\n",
    "    batch = []\n",
    "    cdr_list = []\n",
    "    while len(batch) < 10000:\n",
    "        S = torch.multinomial(aaratio[1:], num_samples=7+5+9, replacement=True) + 1\n",
    "        cdr = [ALPHABET[aa] for aa in S.tolist()]\n",
    "        cdr = ''.join(cdr)\n",
    "        cdr1, cdr2, cdr3 = cdr[:7], cdr[7:12], cdr[12:]\n",
    "        if has_no_repeat(cdr) and cdr1.count('W') + cdr1.count('Y') <= 2 and cdr2.count('W') + cdr2.count('Y') <= 1 and cdr3.count('W') + cdr3.count('Y') <= 2:\n",
    "            batch.append(S)\n",
    "            cdr_list.append((cdr1, cdr2, cdr3))\n",
    "\n",
    "    S = torch.stack(batch, dim=0).cuda()\n",
    "    Y = torch.zeros(len(batch)).cuda()\n",
    "    with torch.no_grad():\n",
    "        prob = model.predict(S)\n",
    "    \n",
    "    for (cdr1, cdr2, cdr3), score in zip(cdr_list, prob.tolist()):\n",
    "        if score > 0.8:\n",
    "            seq = frame1 + cdr1 + frame2 + cdr2 + frame3 + cdr3 + frame4\n",
    "            print(f'{score:.4f} {cdr1} {cdr2} {cdr3} {seq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
